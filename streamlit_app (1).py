
import streamlit as st
import os
import re
import io
import base64
import docx2txt
import fitz  # PyMuPDF
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

st.set_page_config(page_title="Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„Ø£Ø­ÙƒØ§Ù… Ø§Ù„Ù‚Ø¶Ø§Ø¦ÙŠØ©", layout="wide")
st.title("ğŸ” Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø°ÙƒÙŠ ÙÙŠ Ø§Ù„Ø£Ø­ÙƒØ§Ù… Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ÙˆØµÙ")

# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„ÙØ§Øª
uploaded_files = st.file_uploader("ğŸ“¤ Ù‚Ù… Ø¨Ø±ÙØ¹ Ù…Ù„ÙØ§Øª PDF Ù„Ù„Ø£Ø­ÙƒØ§Ù… Ø§Ù„Ù‚Ø¶Ø§Ø¦ÙŠØ©:", type="pdf", accept_multiple_files=True)

# Ø¥Ø¯Ø®Ø§Ù„ Ø§Ù„ÙˆØµÙ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ
query_text = st.text_area("ğŸ“ Ø£Ø¯Ø®Ù„ ÙˆØµÙ Ø§Ù„Ù‚Ø¶ÙŠØ© Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ø¨Ø¯Ù‚Ø©:", height=150, placeholder="Ù…Ø«Ø§Ù„: Ø£Ø­ÙƒØ§Ù… ØªØ±ÙØ¶ Ø¯Ø¹Ø§ÙˆÙ‰ ÙØ±Ø² ÙˆØªØ¬Ù†ÙŠØ¨ Ø¥Ø°Ø§ Ø«Ø¨Øª ÙˆØ¬ÙˆØ¯ Ù‚Ø³Ù…Ø© Ø±Ø¶Ø§Ø¦ÙŠØ© ÙØ¹Ù„ÙŠØ©...")

# Ø¥Ø¯Ø®Ø§Ù„ ÙƒÙ„Ù…Ø§Øª Ù…ÙØªØ§Ø­ÙŠØ© (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)
keywords = st.text_input("ğŸ”‘ ÙƒÙ„Ù…Ø§Øª Ù…ÙØªØ§Ø­ÙŠØ© (Ø§Ø®ØªÙŠØ§Ø±ÙŠ):", placeholder="ÙØ±Ø²ØŒ ØªØ¬Ù†ÙŠØ¨ØŒ Ù‚Ø³Ù…Ø©ØŒ Ø±Ø¶Ø§Ø¦ÙŠØ©")

# Ø²Ø± ØªÙ†ÙÙŠØ° Ø§Ù„Ø¨Ø­Ø«
if st.button("ğŸ” ØªÙ†ÙÙŠØ° Ø§Ù„Ø¨Ø­Ø«") and uploaded_files and query_text:

    def extract_text_from_pdf(file):
        try:
            doc = fitz.open(stream=file.read(), filetype="pdf")
            full_text = ""
            for page in doc:
                full_text += page.get_text()
            return full_text
        except Exception as e:
            return ""

    # Ø¬Ù…Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    documents = []
    filenames = []
    for file in uploaded_files:
        text = extract_text_from_pdf(file)
        if text:
            paragraphs = [p.strip() for p in re.split(r'[\n\r]{2,}|\n', text) if len(p.strip()) > 30]
            for para in paragraphs:
                documents.append(para)
                filenames.append(file.name)

    # Ø¥Ù†Ø´Ø§Ø¡ Ù…ØµÙÙˆÙØ© TF-IDF
    vectorizer = TfidfVectorizer().fit(documents + [query_text])
    doc_vectors = vectorizer.transform(documents)
    query_vector = vectorizer.transform([query_text])

    # Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ´Ø§Ø¨Ù‡
    similarities = cosine_similarity(query_vector, doc_vectors).flatten()
    results = list(zip(filenames, documents, similarities))
    results.sort(key=lambda x: x[2], reverse=True)

    # ÙÙ„ØªØ±Ø© Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø¹Ø§Ù„ÙŠØ© ÙÙ‚Ø·
    threshold = 0.35
    filtered_results = [r for r in results if r[2] >= threshold]

    st.subheader(f"ğŸ“‘ ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {len(filtered_results)} Ù†ØªÙŠØ¬Ø© Ù…ØªØ·Ø§Ø¨Ù‚Ø©:")

    def highlight_keywords(text, terms):
        for word in terms:
            pattern = re.compile(re.escape(word), re.IGNORECASE)
            text = pattern.sub(f"<mark>{word}</mark>", text)
        return text

    search_terms = [w.strip() for w in keywords.split(',')] if keywords else []

    for filename, paragraph, score in filtered_results:
        st.markdown(f"""
        <div style='border:1px solid #ccc; padding:10px; margin-bottom:10px; border-radius:10px;'>
            <b>ğŸ“„ Ø§Ù„Ù…Ù„Ù:</b> {filename}<br>
            <b>ğŸ“ˆ Ø¯Ø±Ø¬Ø© Ø§Ù„ØªØ·Ø§Ø¨Ù‚:</b> {score:.2f}<br>
            <b>ğŸ“Œ Ø§Ù„Ù…Ù‚Ø·Ø¹:</b><br>
            <div style='background-color:#f9f9f9; padding:10px; border-radius:5px;'>
            {highlight_keywords(paragraph, search_terms) if search_terms else paragraph}
            </div>
        </div>
        """, unsafe_allow_html=True)

elif not uploaded_files:
    st.warning("ÙŠØ±Ø¬Ù‰ Ø±ÙØ¹ Ù…Ù„ÙØ§Øª PDF Ø£ÙˆÙ„Ø§Ù‹.")
elif not query_text:
    st.warning("ÙŠØ±Ø¬Ù‰ Ø¥Ø¯Ø®Ø§Ù„ ÙˆØµÙ Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ù„Ù„Ø¨Ø­Ø«.")
