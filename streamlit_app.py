
import streamlit as st
import os
import fitz  # PyMuPDF
import docx
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer
import tempfile

# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØµÙØ­Ø©
st.set_page_config(page_title="Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø°ÙƒÙŠ ÙÙŠ Ø£Ø­ÙƒØ§Ù… Ø§Ù„Ù…Ø­ÙƒÙ…Ø© Ø§Ù„Ø¹Ù„ÙŠØ§", layout="wide")

st.title("ðŸ§  Ø£Ø¯Ø§Ø© Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø°ÙƒÙŠ ÙÙŠ Ø£Ø­ÙƒØ§Ù… Ø§Ù„Ù…Ø­ÙƒÙ…Ø© Ø§Ù„Ø¹Ù„ÙŠØ§")
st.write("ðŸ” Ø§Ø¨Ø­Ø« Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ ÙˆØµÙ Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ø¯Ù‚ÙŠÙ‚ Ø¯Ø§Ø®Ù„ Ù…Ù„ÙØ§Øª Word Ø£Ùˆ PDF.")

uploaded_files = st.file_uploader("ðŸ“‚ Ø§Ø±ÙØ¹ Ù…Ù„ÙØ§Øª Word Ø£Ùˆ PDF", type=["docx", "pdf"], accept_multiple_files=True)

query = st.text_area("ðŸ“ Ø£Ø¯Ø®Ù„ ÙˆØµÙ Ø§Ù„Ù‚Ø¶ÙŠØ© Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ø¨Ø¯Ù‚Ø©:", height=150)
keywords = st.text_input("ðŸ”‘ ÙƒÙ„Ù…Ø§Øª Ù…ÙØªØ§Ø­ÙŠØ© (Ø§Ø®ØªÙŠØ§Ø±ÙŠ):")

start_search = st.button("ðŸ”Ž ØªÙ†ÙÙŠØ° Ø§Ù„Ø¨Ø­Ø«")

def extract_text_from_pdf(file):
    text = ""
    try:
        with fitz.open(stream=file.read(), filetype="pdf") as doc:
            for page in doc:
                text += page.get_text()
    except:
        text = ""
    return text

def extract_text_from_docx(file):
    with tempfile.NamedTemporaryFile(delete=False, suffix=".docx") as tmp:
        tmp.write(file.read())
        tmp_path = tmp.name
    doc = docx.Document(tmp_path)
    text = "\n".join([para.text for para in doc.paragraphs])
    os.unlink(tmp_path)
    return text

if start_search and query and uploaded_files:
    with st.spinner("Ø¬Ø§Ø±ÙŠ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©..."):
        model = SentenceTransformer("all-MiniLM-L6-v2")
        results = []

        for file in uploaded_files:
            filename = file.name
            if filename.endswith(".pdf"):
                text = extract_text_from_pdf(file)
            else:
                text = extract_text_from_docx(file)

            paragraphs = [p.strip() for p in text.split("\n") if len(p.strip()) > 30]
            if not paragraphs:
                continue

            para_embeddings = model.encode(paragraphs)
            query_embedding = model.encode([query])

            similarities = cosine_similarity(query_embedding, para_embeddings)[0]
            top_indices = similarities.argsort()[-3:][::-1]

            for idx in top_indices:
                sim_score = similarities[idx]
                if sim_score > 0.45:
                    results.append({
                        "filename": filename,
                        "score": round(sim_score, 2),
                        "snippet": paragraphs[idx]
                    })

    if results:
        st.success(f"ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {len(results)} Ù†ØªÙŠØ¬Ø© Ù…Ø´Ø§Ø¨Ù‡Ø©:")
        for res in sorted(results, key=lambda x: x["score"], reverse=True):
            st.markdown(f"""ðŸ“„ **{res['filename']}**
            ðŸ”¥ Ø¯Ø±Ø¬Ø© Ø§Ù„ØªØ·Ø§Ø¨Ù‚: {res['score']}
            ðŸ“Œ Ø§Ù„Ù…Ù‚Ø·Ø¹:  
            > {res['snippet']}
            ---
            """)
    else:
        st.warning("Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù†ØªØ§Ø¦Ø¬ Ù…ØªØ·Ø§Ø¨Ù‚Ø©.")
