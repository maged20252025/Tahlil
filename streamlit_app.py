
import streamlit as st
import os
import docx2txt
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import re

# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØµÙØ­Ø©
st.set_page_config(page_title="Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø£Ø­ÙƒØ§Ù… Ø§Ù„Ù…Ø­ÙƒÙ…Ø© Ø§Ù„Ø¹Ù„ÙŠØ§", layout="wide")

# Ø¹Ù†ÙˆØ§Ù† Ø§Ù„ØªØ·Ø¨ÙŠÙ‚
st.title("ğŸ“š Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø£Ø­ÙƒØ§Ù… Ø§Ù„Ù…Ø­ÙƒÙ…Ø© Ø§Ù„Ø¹Ù„ÙŠØ§")

# Ø£Ø²Ø±Ø§Ø± Ø§Ù„ØªÙ†Ù‚Ù„
st.markdown("""
    <style>
    .fixed-buttons {
        position: fixed;
        top: 80px;
        right: 20px;
        z-index: 9999;
    }
    .fixed-buttons button {
        margin: 5px 0;
    }
    </style>
    <div class="fixed-buttons">
        <a href="#top"><button>ğŸ”¼ Ø¥Ù„Ù‰ Ø§Ù„Ø£Ø¹Ù„Ù‰</button></a>
        <a href="#bottom"><button>ğŸ”½ Ø¥Ù„Ù‰ Ø§Ù„Ø£Ø³ÙÙ„</button></a>
    </div>
    <div id="top"></div>
""", unsafe_allow_html=True)

# Ø­Ù‚Ù„ Ø¥Ø¯Ø®Ø§Ù„ Ø§Ù„Ù†Øµ
query = st.text_area("ğŸ” Ø§ÙƒØªØ¨ ÙˆØµÙ Ø§Ù„Ù‚Ø¶ÙŠØ© Ø£Ùˆ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©")

# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„ÙØ§Øª Ù…Ù† Ù…Ø¬Ù„Ø¯ Ø«Ø§Ø¨Øª Ù…Ø³Ø¨Ù‚Ù‹Ø§
BASE_DIR = "data"
documents = []
file_names = []

# Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù„ÙØ§Øª Ù…Ù† Ø§Ù„Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø«Ø§Ø¨Øª
for root, dirs, files in os.walk(BASE_DIR):
    for file in files:
        if file.endswith(".docx"):
            file_path = os.path.join(root, file)
            text = docx2txt.process(file_path)
            documents.append(text)
            file_names.append(file)

# ØªØ§Ø¨Ø¹ Ø§Ù„Ø¨Ø­Ø«
def search_documents(query, documents, file_names):
    vectorizer = TfidfVectorizer().fit_transform([query] + documents)
    cosine_similarities = cosine_similarity(vectorizer[0:1], vectorizer[1:]).flatten()
    results = sorted(zip(file_names, documents, cosine_similarities), key=lambda x: x[2], reverse=True)
    return results

# ØªÙ†ÙÙŠØ° Ø§Ù„Ø¨Ø­Ø« Ø¹Ù†Ø¯ Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„
if query:
    results = search_documents(query, documents, file_names)
    found = False
    for i, (name, content, score) in enumerate(results):
        if score > 0:
            found = True
            st.markdown(f"### ğŸ”¹ Ù†ØªÙŠØ¬Ø© Ø±Ù‚Ù… {i+1} - Ø§Ù„Ù…Ù„Ù: `{name}`")
            paragraphs = content.split("\n")
            for para in paragraphs:
                if re.search(query, para, re.IGNORECASE):
                    highlighted = re.sub(f"({query})", r"<mark>\1</mark>", para, flags=re.IGNORECASE)
                    st.markdown(f"<p style='background-color:#f9f9f9;padding:10px;border-radius:5px'>{highlighted}</p>", unsafe_allow_html=True)
    if not found:
        st.warning("Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø£ÙŠ Ù†ØªÙŠØ¬Ø© Ù…Ø·Ø§Ø¨Ù‚Ø©.")
    st.markdown("<div id='bottom'></div>", unsafe_allow_html=True)
else:
    st.info("ÙŠØ±Ø¬Ù‰ Ø¥Ø¯Ø®Ø§Ù„ ÙˆØµÙ Ø§Ù„Ù‚Ø¶ÙŠØ© Ø£Ùˆ ÙƒÙ„Ù…Ø§Øª Ù…ÙØªØ§Ø­ÙŠØ© Ù„Ù„Ø¨Ø­Ø«.")
